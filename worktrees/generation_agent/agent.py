import json
import os
import sys

base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if base_dir not in sys.path:
    sys.path.append(base_dir)

from src.utils.llm_router import get_llm

try:
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import JsonOutputParser
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


class GenerationAgent:
    """
    ìƒì„±/ê´€ë¦¬ ì—ì´ì „íŠ¸: 
    1) ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆëŠ” ì»´í¬ë„ŒíŠ¸ëŠ” ì¦‰ì‹œ ë°˜í™˜ (ìºì‹±/ì‚¬ì „ ì •ì˜)
    2) ì—†ëŠ” ì»´í¬ë„ŒíŠ¸ëŠ” LLMì„ í†µí•´ ìµœì†Œ ë‹¨ìœ„(Atomic)ë¡œ ë™ì  ìƒì„± í›„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì €ì¥.
    """
    def __init__(self):
        self.name = "GenerationAgent"
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        self.library_path = os.path.join(base_dir, "output", "components")
        os.makedirs(self.library_path, exist_ok=True)

        provider = os.getenv("GENERATION_LLM_PROVIDER", os.getenv("AI_PROVIDER", "openai"))
        model_name = os.getenv("GENERATION_LLM_MODEL", os.getenv("AI_MODEL", "gpt-4o"))
        self.llm = get_llm(provider=provider, model_name=model_name)

        if LANGCHAIN_AVAILABLE:
            self.parser = JsonOutputParser()
            self.prompt = PromptTemplate(
                template=(
                    "You are an expert frontend developer.\n"
                    "Generate a minimal, reusable HTML UI component named '{component_name}' using Tailwind CSS classes.\n"
                    "Requirements:\n"
                    "- Do NOT include <html>, <head>, or <body> tags.\n"
                    "- Output ONLY the component's HTML code.\n"
                    "- Do NOT include any background colors on the outermost layer of the component so it blends naturally.\n"
                    "- Use generic placeholders like {{text}} or {{title}} for content that should be dynamic.\n"
                    "Return the response strictly as a JSON object with this exact format, without any markdown formatting wrappers:\n"
                    "{{\n"
                    '  "type": "component",\n'
                    '  "name": "{component_name}",\n'
                    '  "html_template": "...",\n'
                    '  "required_params": ["list of variables like text, title"],\n'
                    '  "description": "brief explanation"\n'
                    "}}\n"
                ),
                input_variables=["component_name"]
            )
            self.chain = self.prompt | self.llm | self.parser
        else:
            self.chain = None

        # MVP í…ŒìŠ¤íŠ¸ìš© í•˜ë“œì½”ë”©ëœ ëª¨ì˜ LLM ì‘ë‹µê¸°
        self._mock_llm_responses = {
            "text_input": {
                "type": "component",
                "name": "text_input",
                "html_template": "<input type=\"text\" class=\"border p-2 rounded w-full\" placeholder=\"{placeholder}\" />",
                "required_params": ["placeholder"],
                "description": "A dynamic text input generated by LLM."
            },
            "custom_graph": {
                "type": "component",
                "name": "custom_graph",
                "html_template": "<div class=\"w-full h-32 bg-gray-200 border-2 border-dashed border-gray-400 flex items-center justify-center\"><span class=\"text-gray-500\">Dynamic Graph Area for {data_type}</span></div>",
                "required_params": ["data_type"],
                "description": "A dynamically generated atomic graph placeholder."
            }
        }

    def load_component_metadata(self, component_name: str) -> dict:
        file_path = os.path.join(self.library_path, f"{component_name}.json")
        
        # 1. ë¼ì´ë¸ŒëŸ¬ë¦¬(ìºì‹œ) í™•ì¸ ë¡œì§
        if os.path.exists(file_path):
            print(f"[{self.name}] ğŸŸ¢ ë¼ì´ë¸ŒëŸ¬ë¦¬ íˆíŠ¸: '{component_name}' (ì‚¬ì „ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ)")
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        # 2. ë™ì  ìƒì„± (Atomic Component) ë¡œì§
        print(f"[{self.name}] ğŸŸ¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ìŠ¤: '{component_name}' (LLM ìµœì†Œ ë‹¨ìœ„ ë™ì  ìƒì„± ì‹œì‘)")
        
        dynamic_component = self._call_llm_for_atomic_component(component_name)
        
        # 3. ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì €ì¥ (ìºì‹±)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(dynamic_component, f, indent=4, ensure_ascii=False)
        print(f"[{self.name}] ğŸ’¾ ë™ì  ì»´í¬ë„ŒíŠ¸ ì €ì¥ ì™„ë£Œ: '{component_name}'")
        
        return dynamic_component

    def _call_llm_for_atomic_component(self, name: str) -> dict:
        # ì‹¤ì œ LLM í˜¸ì¶œ
        if LANGCHAIN_AVAILABLE and self.chain:
            try:
                response = self.chain.invoke({"component_name": name})
                return response
            except Exception as e:
                print(f"[{self.name}] LLM ì²´ì¸ ì‹¤íŒ¨, Fallback ëª¨ì˜ ë°ì´í„° ë°˜í™˜: {e}")
                
        # Fallback (ì „í˜€ ëª¨ë¥´ëŠ” ì»´í¬ë„ŒíŠ¸ì¼ ê²½ìš° ë˜ëŠ” LLM/ëª¨ì˜ ê°ì²´ ì‹¤íŒ¨ ì‹œ)
        if name in self._mock_llm_responses:
            return self._mock_llm_responses[name]
            
        return {
            "type": "component",
            "name": name,
            "html_template": f"<div class=\"p-4 bg-yellow-100 border border-yellow-300 rounded\">Generated Component: {name} (Variable: {'{'}param{'}'})</div>",
            "required_params": ["param"],
            "description": f"Fallback LLM generated component for {name}."
        }
