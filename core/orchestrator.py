import atexit
import concurrent.futures
import json
import os
import signal
import sys
import threading
import time
import uuid
import weakref
from datetime import datetime, timezone

# í”„ë¡œì íŠ¸ ë£¨íŠ¸(coreì˜ ìƒìœ„)ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ worktrees ë‚´ë¶€ì˜ ëª¨ë“ˆì„ absolute pathì²˜ëŸ¼ ì°¸ì¡° ê°€ëŠ¥í•˜ê²Œ í•¨
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from worktrees.customer_agent.agent import CustomerAgent
from worktrees.generation_agent.agent import GenerationAgent
from worktrees.composition_agent.agent import CompositionAgent
from worktrees.methodology_agent.agent import MethodologyAgent
from scripts.git_manager import GitManager

class Telemetry:
    """GSD ì²´ê³„ í•˜ì—ì„œ ì»´í¬ë„ŒíŠ¸ ì²˜ë¦¬ íš¨ìœ¨ì„±(í† í° ì ˆê°)ì„ ê¸°ë¡í•˜ëŠ” ëª¨ë“ˆ"""
    def __init__(self):
        self.total_requested = 0
        self.cache_hits = 0
        self.llm_generations = 0

    def record_hit(self):
        self.cache_hits += 1
        self.total_requested += 1

    def record_miss(self):
        self.llm_generations += 1
        self.total_requested += 1

    def get_efficiency_rate(self) -> float:
        if self.total_requested == 0: return 0.0
        return (self.cache_hits / self.total_requested) * 100

    def generate_dashboard_html(self, phase, output_path=None):
        if output_path is None:
            output_path = os.path.join(os.path.dirname(__file__), '..', 'output', 'dashboard.html')
            
        efficiency = self.get_efficiency_rate()
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Builder - Token Efficiency Dashboard</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 p-8 font-sans text-gray-800">
    <div class="max-w-3xl mx-auto bg-white rounded-xl shadow-lg p-6">
        <h1 class="text-3xl font-bold mb-2">ğŸš€ AI Builder Telemetry Dashboard</h1>
        <p class="text-gray-500 mb-6">Current Phase: <span class="font-semibold text-blue-600">{phase}</span></p>
        
        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-8">
            <div class="bg-blue-50 p-4 rounded-lg border border-blue-100 text-center">
                <div class="text-sm text-blue-500 font-bold uppercase tracking-wide">Total Components</div>
                <div class="text-4xl font-extrabold text-blue-700 mt-2">{self.total_requested}</div>
            </div>
            <div class="bg-green-50 p-4 rounded-lg border border-green-100 text-center">
                <div class="text-sm text-green-500 font-bold uppercase tracking-wide">Cache Hits (Tokens Saved)</div>
                <div class="text-4xl font-extrabold text-green-700 mt-2">{self.cache_hits}</div>
            </div>
            <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-100 text-center">
                <div class="text-sm text-yellow-500 font-bold uppercase tracking-wide">LLM Generations</div>
                <div class="text-4xl font-extrabold text-yellow-700 mt-2">{self.llm_generations}</div>
            </div>
        </div>

        <div class="mb-4">
            <h2 class="text-xl font-bold mb-2">Token Savings Efficiency</h2>
            <div class="w-full bg-gray-200 rounded-full h-6 overflow-hidden">
                <div class="bg-gradient-to-r from-green-400 to-green-600 h-6 text-xs font-bold text-white text-center p-1 leading-none transition-all duration-1000" style="width: {efficiency}%">
                    {efficiency:.1f}% Cached
                </div>
            </div>
            <p class="text-sm text-gray-500 mt-2">Target savings rate: >50% (GSD Standard)</p>
        </div>
        
        <div class="mt-8 text-sm text-gray-400 border-t pt-4">
            * Dashboard updated automatically by Orchestrator.
        </div>
    </div>
</body>
</html>"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        return output_path

class Orchestrator:
    """
    ìƒì• ì£¼ê¸° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: GDS ë‹¨ê³„ì— ë”°ë¼ ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ ì œì–´
    ì´ë²ˆ í…ŒìŠ¤íŠ¸: ë™ì  ì»´í¬ë„ŒíŠ¸(custom_graph, text_input ë“±) ìƒì„± ë° í†µí•© ë¡œì§
    """
    _hooks_lock = threading.Lock()
    _instances = weakref.WeakSet()
    _hooks_registered = False
    _startup_recovery_done = False
    _startup_recovery_running = False
    _previous_signal_handlers = {}
    _last_signal = None

    def __init__(self, config_path=None):
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'lifecycle_config.json')
            
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
            
        self.phase = self.config['current_phase']
        self.phase_metrics = self.config['phases'][self.phase]['gate_metrics']
        
        self.customer = CustomerAgent()
        self.generator = GenerationAgent()
        self.composer = CompositionAgent()
        self.methodology = MethodologyAgent()
        
        self.repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
        self.runtime_output_dir = os.path.abspath(
            os.getenv("RUNTIME_OUTPUT_DIR", os.path.join(self.repo_root, "output", "runtime"))
        )
        os.makedirs(self.runtime_output_dir, exist_ok=True)
        self.disable_merge_to_main = self._env_flag("ORCHESTRATOR_DISABLE_MERGE", default=False)

        self.telemetry = Telemetry()
        self.git_manager = GitManager(self.repo_root)
        self.journal_dir = os.path.join(self.runtime_output_dir, "orchestrator_runs")
        os.makedirs(self.journal_dir, exist_ok=True)

        self._state_lock = threading.Lock()
        self._active_resources = {}
        self._run_journal_paths = {}

        self._register_process_hooks()
        self._run_startup_recovery_once()

    @staticmethod
    def _env_flag(name: str, default: bool = False) -> bool:
        raw = os.getenv(name)
        if raw is None:
            return default

        normalized = str(raw).strip().lower()
        if normalized in ("1", "true", "yes", "on"):
            return True
        if normalized in ("0", "false", "no", "off"):
            return False
        return default

    @staticmethod
    def _utcnow_iso() -> str:
        return datetime.now(timezone.utc).isoformat()

    @classmethod
    def _handle_atexit(cls):
        reason = "atexit"
        if cls._last_signal is not None:
            reason = f"signal:{cls._last_signal}"

        for instance in list(cls._instances):
            instance._cleanup_all_active_resources(reason)

    @classmethod
    def _handle_signal(cls, signum, frame):
        # ì‹ í˜¸ í•¸ë“¤ëŸ¬ì—ì„œëŠ” ë½ì„ ì¡ëŠ” ì •ë¦¬ ë¡œì§ì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ.
        # ì‹¤ì œ ì •ë¦¬ëŠ” run_pipeline finally ë˜ëŠ” atexitì—ì„œ ìˆ˜í–‰ëœë‹¤.
        cls._last_signal = signum

        previous_handler = cls._previous_signal_handlers.get(signum)
        if callable(previous_handler) and previous_handler is not cls._handle_signal:
            previous_handler(signum, frame)
            return

        if signum == signal.SIGINT:
            raise KeyboardInterrupt
        raise SystemExit(0)

    def _register_process_hooks(self):
        with Orchestrator._hooks_lock:
            Orchestrator._instances.add(self)

            if Orchestrator._hooks_registered:
                return

            if threading.current_thread() is threading.main_thread():
                for signum in (signal.SIGINT, signal.SIGTERM):
                    Orchestrator._previous_signal_handlers[signum] = signal.getsignal(signum)
                    signal.signal(signum, Orchestrator._handle_signal)

            atexit.register(Orchestrator._handle_atexit)
            Orchestrator._hooks_registered = True

    def _run_startup_recovery_once(self):
        with Orchestrator._hooks_lock:
            if Orchestrator._startup_recovery_done:
                return
            if Orchestrator._startup_recovery_running:
                return
            Orchestrator._startup_recovery_running = True

        recovery_success = False
        try:
            self._recover_stale_worktrees()
            recovery_success = True
        finally:
            with Orchestrator._hooks_lock:
                if recovery_success:
                    Orchestrator._startup_recovery_done = True
                Orchestrator._startup_recovery_running = False

    def _write_json_atomic(self, path: str, payload: dict):
        temp_path = f"{path}.tmp"
        with open(temp_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        os.replace(temp_path, path)

    def _create_run_id(self) -> str:
        return f"run_{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}"

    def _run_journal_path(self, run_id: str) -> str:
        return os.path.join(self.journal_dir, f"{run_id}.json")

    def _snapshot_run_resources(self, run_id: str) -> list:
        with self._state_lock:
            resources = [
                dict(resource)
                for resource in self._active_resources.values()
                if resource.get("run_id") == run_id
            ]
        resources.sort(key=lambda item: (item.get("component") or "", item.get("branch_name") or ""))
        return resources

    def _start_run_journal(self, session_id: str, user_request: str) -> str:
        run_id = self._create_run_id()
        journal_path = self._run_journal_path(run_id)
        with self._state_lock:
            self._run_journal_paths[run_id] = journal_path

        payload = {
            "run_id": run_id,
            "session_id": session_id,
            "status": "running",
            "request": user_request,
            "created_at": self._utcnow_iso(),
            "updated_at": self._utcnow_iso(),
            "finished_at": None,
            "error": None,
            "resources": [],
        }
        self._write_json_atomic(journal_path, payload)
        return run_id

    def _update_run_journal(self, run_id: str, status: str = None, error_marker: str = None, finished: bool = False):
        with self._state_lock:
            journal_path = self._run_journal_paths.get(run_id, self._run_journal_path(run_id))
            self._run_journal_paths[run_id] = journal_path

        try:
            if os.path.exists(journal_path):
                with open(journal_path, 'r', encoding='utf-8') as f:
                    payload = json.load(f)
            else:
                payload = {"run_id": run_id}
        except Exception:
            payload = {"run_id": run_id}

        payload["updated_at"] = self._utcnow_iso()
        payload["resources"] = self._snapshot_run_resources(run_id)

        if status is not None:
            payload["status"] = status
        if error_marker is not None:
            payload["error"] = error_marker
        if finished:
            payload["finished_at"] = self._utcnow_iso()

        self._write_json_atomic(journal_path, payload)

    def _build_component_resource(self, comp: str):
        branch_name = f"feat/{comp}_gen"
        worktree_path = os.path.join(self.repo_root, 'worktrees', f"temp_{comp}")
        return branch_name, worktree_path

    def _track_resource(self, run_id: str, component: str, branch_name: str, worktree_path: str):
        normalized_path = os.path.abspath(worktree_path)
        resource_key = (branch_name, normalized_path)
        resource_value = {
            "run_id": run_id,
            "component": component,
            "branch_name": branch_name,
            "worktree_path": normalized_path,
            "tracked_at": self._utcnow_iso(),
        }
        with self._state_lock:
            self._active_resources[resource_key] = resource_value
        self._update_run_journal(run_id)

    def _untrack_resource(self, branch_name: str, worktree_path: str):
        normalized_path = os.path.abspath(worktree_path)
        resource_key = (branch_name, normalized_path)
        run_id = None

        with self._state_lock:
            existing = self._active_resources.pop(resource_key, None)
            if existing:
                run_id = existing.get("run_id")

        if run_id:
            self._update_run_journal(run_id)

    def _safe_remove_resource(self, branch_name: str, worktree_path: str, context: str):
        try:
            self.git_manager.remove_worktree(worktree_path, branch_name=branch_name, force=True)
        except Exception as exc:
            print(f"[Cleanup] Warning ({context}) {branch_name}: {exc}")
        finally:
            self._untrack_resource(branch_name, worktree_path)

    def _cleanup_run_resources(self, run_id: str, reason: str):
        resources = self._snapshot_run_resources(run_id)
        if not resources:
            return

        print(f"[Cleanup] run_id={run_id}, count={len(resources)}, reason={reason}")
        for resource in resources:
            self._safe_remove_resource(
                resource.get("branch_name"),
                resource.get("worktree_path"),
                context=reason,
            )

    def _cleanup_all_active_resources(self, reason: str):
        with self._state_lock:
            run_ids = sorted({entry.get("run_id") for entry in self._active_resources.values() if entry.get("run_id")})

        for run_id in run_ids:
            self._cleanup_run_resources(run_id, f"global-{reason}")
            self._update_run_journal(
                run_id,
                status="interrupted",
                error_marker=f"Cleanup triggered by {reason}",
                finished=True,
            )

    def _recover_stale_worktrees(self):
        recovered_count = 0

        journal_dirs = []
        for candidate in (
            self.journal_dir,
            os.path.join(self.repo_root, "output", "orchestrator_runs"),
        ):
            normalized = os.path.abspath(candidate)
            if normalized not in journal_dirs and os.path.isdir(normalized):
                journal_dirs.append(normalized)

        # 1) ì‹¤í–‰ ì¤‘ ìƒíƒœë¡œ ë‚¨ì•„ìˆëŠ” ì €ë„ ê¸°ë°˜ ë³µêµ¬
        for journal_dir in journal_dirs:
            for file_name in sorted(os.listdir(journal_dir)):
                if not file_name.endswith(".json"):
                    continue

                journal_path = os.path.join(journal_dir, file_name)
                try:
                    with open(journal_path, 'r', encoding='utf-8') as f:
                        payload = json.load(f)
                except Exception:
                    continue

                if payload.get("status") != "running":
                    continue

                failed_resources = []
                for resource in payload.get("resources", []):
                    branch_name = resource.get("branch_name")
                    worktree_path = resource.get("worktree_path")
                    if not worktree_path:
                        continue
                    try:
                        self.git_manager.remove_worktree(worktree_path, branch_name=branch_name, force=True)
                        recovered_count += 1
                    except Exception as exc:
                        resource_copy = dict(resource)
                        resource_copy["last_error"] = str(exc)
                        failed_resources.append(resource_copy)

                if failed_resources:
                    payload["status"] = "partial_recovered"
                    payload["error"] = f"Recovered with {len(failed_resources)} resource(s) still failing cleanup."
                    payload["resources"] = failed_resources
                else:
                    payload["status"] = "recovered"
                    payload["error"] = "Recovered stale resources during startup."
                    payload["resources"] = []
                payload["updated_at"] = self._utcnow_iso()
                payload["finished_at"] = self._utcnow_iso()
                self._write_json_atomic(journal_path, payload)

        # 2) ì €ë„ì— ì—†ëŠ” temp_* ì›Œí¬íŠ¸ë¦¬ ë³µêµ¬
        stale_paths = self.git_manager.cleanup_stale_temp_worktrees()
        recovered_count += len(stale_paths)

        if recovered_count:
            print(f"[Recovery] stale worktrees cleaned: {recovered_count}")

    def _resolve_worker_count(self, env_key: str, default: int) -> int:
        try:
            value = int(os.getenv(env_key, str(default)))
        except (TypeError, ValueError):
            value = default
        return max(1, value)

    def _get_agent_thread_pool_config(self) -> dict:
        return {
            "customer": self._resolve_worker_count("CUSTOMER_AGENT_THREADS", 1),
            "generation": self._resolve_worker_count("GENERATION_AGENT_THREADS", 5),
            "methodology": self._resolve_worker_count("METHODOLOGY_AGENT_THREADS", 5),
            "composition": self._resolve_worker_count("COMPOSITION_AGENT_THREADS", 1),
        }

    def _generate_component_worker(self, comp: str):
        branch_name, worktree_path = self._build_component_resource(comp)
        
        # 1. ì›Œí¬íŠ¸ë¦¬ ìƒì„±
        try:
            self.git_manager.add_worktree(branch_name, worktree_path)
        except Exception:
            pass # ignore if already exists/fails
            
        # 2. GenerationAgent ì—°ì‚° ìˆ˜í–‰
        file_path = os.path.join(self.generator.library_path, f"{comp}.json")
        is_hit = os.path.exists(file_path)
        
        meta = self.generator.load_component_metadata(comp)
        
        # 3. ì›Œí¬íŠ¸ë¦¬ ë‚´ì— íŒŒì¼ ì €ì¥ ë° ì»¤ë°‹
        if os.path.exists(worktree_path):
            comp_file = os.path.join(worktree_path, f"{comp}.json")
            with open(comp_file, 'w', encoding='utf-8') as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
                
            try:
                self.git_manager.commit_changes(worktree_path, f"Generation Agent: Created {comp}")
            except Exception:
                pass # ì•„ë¬´ ë³€ê²½ì‚¬í•­ ì—†ìŒ
                
        return comp, meta, is_hit, branch_name, worktree_path

    def run_pipeline(self, session_id: str, user_request: str):
        run_id = self._start_run_journal(session_id, user_request)
        run_status = "failed"
        run_error = None

        print(f"\n==========================================")
        print(f"ğŸš€ AI BUILDER ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œì‘ [Phase: {self.phase}]")
        print(f"==========================================")
        
        try:
            thread_pool = self._get_agent_thread_pool_config()
            print(
                "[ThreadPool] "
                f"Customer={thread_pool['customer']}, "
                f"Generation={thread_pool['generation']}, "
                f"Methodology={thread_pool['methodology']}, "
                f"Composition={thread_pool['composition']}"
            )
            if self.disable_merge_to_main:
                print("[Safety] ORCHESTRATOR_DISABLE_MERGE=1 -> main ë¸Œëœì¹˜ ë³‘í•© ë¹„í™œì„±í™”")

            with concurrent.futures.ThreadPoolExecutor(
                max_workers=thread_pool["customer"],
                thread_name_prefix="CustomerAgent",
            ) as customer_executor, concurrent.futures.ThreadPoolExecutor(
                max_workers=thread_pool["generation"],
                thread_name_prefix="GenerationAgent",
            ) as generation_executor, concurrent.futures.ThreadPoolExecutor(
                max_workers=thread_pool["methodology"],
                thread_name_prefix="MethodologyAgent",
            ) as methodology_executor:
                # 1. Customer Agent: íŒŒì‹±
                parsed_data = customer_executor.submit(
                    self.customer.process_request, session_id, user_request
                ).result()
                components_needed = parsed_data["required_components"]

                for comp in components_needed:
                    branch_name, worktree_path = self._build_component_resource(comp)
                    self._track_resource(run_id, comp, branch_name, worktree_path)

                if self.phase == "Alpha" and len(components_needed) > self.phase_metrics.get("max_components_allowed", 10):
                    print(f"[Error] Alpha ë‹¨ê³„ í—ˆìš© ì»´í¬ë„ŒíŠ¸ ì´ˆê³¼: {len(components_needed)}")
                    run_status = "blocked"
                    return None

                # 2. Generation + Methodology Agent: ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
                print(f"\nâš¡ [Generation Agent] {len(components_needed)}ê°œ ì»´í¬ë„ŒíŠ¸ ë³‘ë ¬ ìƒì„± ì‹œì‘...")

                generation_futures = {
                    generation_executor.submit(self._generate_component_worker, comp): (index, comp)
                    for index, comp in enumerate(components_needed)
                }
                qa_futures = {}

                for generation_future in concurrent.futures.as_completed(generation_futures):
                    index, comp = generation_futures[generation_future]
                    try:
                        _, meta, is_hit, branch_name, worktree_path = generation_future.result()
                    except Exception as exc:
                        print(f"   [Error] {comp} ì‘ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
                        continue

                    print(f"   [!] Methodology Agent inspecting {comp}...")
                    qa_future = methodology_executor.submit(self.methodology.process, meta)
                    qa_futures[qa_future] = (index, comp, meta, is_hit, branch_name, worktree_path)

                approved_results = []
                for qa_future in concurrent.futures.as_completed(qa_futures):
                    index, comp, meta, is_hit, branch_name, worktree_path = qa_futures[qa_future]

                    try:
                        qa_result = qa_future.result()
                    except Exception as exc:
                        qa_result = {"status": "failed", "reason": f"QA ì˜ˆì™¸: {exc}"}

                    if qa_result.get("status") == "failed":
                        print(f"   [Error] {comp} QA Failed: {qa_result.get('reason')}. Skipping merge.")
                        self._safe_remove_resource(branch_name, worktree_path, "qa-fail")
                        continue

                    approved_results.append((index, meta, branch_name, worktree_path))
                    if is_hit:
                        self.telemetry.record_hit()
                    else:
                        self.telemetry.record_miss()
                    print(f"   [+] {comp} ì‘ì—… ì™„ë£Œ ë° QA í†µê³¼ (Cache Hit: {is_hit}) | Branch: {branch_name}")

                approved_results.sort(key=lambda item: item[0])
                library_assets = [meta for _, meta, _, _ in approved_results]
                generated_branches = [(branch_name, worktree_path) for _, _, branch_name, worktree_path in approved_results]

            # 3. Composition Agent: ì›ì ì¡°ê° í†µí•© ì¡°ë¦½ (Merge Master ì—­í•  ë³‘í–‰)
            print("\nğŸ”„ [Composition Agent] ë³‘í•© ì¡°ìœ¨ ì‹œì‘ (Merge Master)")
            if self.disable_merge_to_main:
                print("   [Safety] main ë³‘í•©ì„ ê±´ë„ˆë›°ê³  ì›Œí¬íŠ¸ë¦¬/ë¸Œëœì¹˜ë§Œ ì •ë¦¬í•©ë‹ˆë‹¤.")
                for branch_name, worktree_path in generated_branches:
                    if branch_name and worktree_path:
                        self._safe_remove_resource(branch_name, worktree_path, "merge-disabled")
            else:
                for branch_name, worktree_path in generated_branches:
                    if branch_name and worktree_path:
                        print(f"   â®‘ Merging {branch_name}...")
                        try:
                            success, output = self.git_manager.merge_branch(branch_name, allow_unrelated=True)
                            if not success:
                                print(f"      [Warning] Merge conflict for {branch_name} - Composition Agent ê°œì… í•„ìš”. ({output})")
                        except Exception as e:
                            print(f"      [Error] ë³‘í•© ì¤‘ ì—ëŸ¬: {e}")
                        finally:
                            # ë³‘í•© ì™„ë£Œ/ì‹¤íŒ¨ì™€ ë¬´ê´€í•˜ê²Œ ì›Œí¬íŠ¸ë¦¬ ì •ë¦¬
                            self._safe_remove_resource(branch_name, worktree_path, "post-merge")
            
            print("\nâœ¨ Final Layout Composition...")
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=thread_pool["composition"],
                thread_name_prefix="CompositionAgent",
            ) as composition_executor:
                final_code = composition_executor.submit(
                    self.composer.compose, parsed_data, library_assets
                ).result()
            
            # ê²°ê³¼ë¬¼ ì €ì¥
            output_file = os.path.join(self.runtime_output_dir, 'builder_output.html')
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_code)
                
            dashboard_file = self.telemetry.generate_dashboard_html(
                self.phase,
                output_path=os.path.join(self.runtime_output_dir, "dashboard.html"),
            )
            efficiency = self.telemetry.get_efficiency_rate()
                
            print(f"\nâœ… [ê²°ê³¼ë¬¼ ì‚°ì¶œ ì„±ê³µ] íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"ğŸ“Š [ì§€í‘œ ì—…ë°ì´íŠ¸ ì™„ë£Œ] ëŒ€ì‹œë³´ë“œ ì €ì¥ ì™„ë£Œ: {dashboard_file}")
            print(f"   â–º í† í° ì ˆê°ë¥ (Cache Hit): {efficiency:.1f}%")
            print("==========================================\n")

            run_status = "completed"
            # API í˜¸í™˜ì„±ì„ ìœ„í•´ ê²°ê³¼ ì½”ë“œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë¦¬í„´
            return {
                "html": final_code,
                "metrics": {
                    "total": self.telemetry.total_requested,
                    "hits": self.telemetry.cache_hits,
                    "misses": self.telemetry.llm_generations,
                    "efficiency": round(efficiency, 2)
                }
            }
        except BaseException as exc:
            if isinstance(exc, KeyboardInterrupt):
                run_status = "interrupted"
            run_error = f"{type(exc).__name__}: {exc}"
            print(f"[Pipeline] ì˜ˆì™¸ ë°œìƒ: {exc}")
            raise
        finally:
            self._cleanup_run_resources(run_id, "run-finally")
            self._update_run_journal(
                run_id,
                status=run_status,
                error_marker=run_error,
                finished=True,
            )

if __name__ == "__main__":
    orchestrator = Orchestrator()
    # GSD ê²€ì¦ì„ ìœ„í•´ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ê°€ ì„ì¸ ëª¨ì˜ ìš”ì²­
    orchestrator.customer.process_request = lambda s, u: {
        "session_id": s,
        "required_components": ["header", "nav_bar", "hero_section", "custom_graph", "text_input", "unknown_dynamic_widget", "button", "footer_simple"],
        "user_intent": "ê³ ê¸‰ ì—”í„°í”„ë¼ì´ì¦ˆ ëŒ€ì‹œë³´ë“œ í™”ë©´"
    }
    
    sample_request = "ê³ ê¸‰ ëŒ€ì‹œë³´ë“œ ë§Œë“¤ì–´ì¤˜. í—¤ë”, ë„¤ë¹„, íˆì–´ë¡œ, ê·¸ë˜í”„, í…ìŠ¤íŠ¸ì…ë ¥, ì•Œìˆ˜ì—†ëŠ”ìœ„ì ¯, ë²„íŠ¼, í‘¸í„° ë‹¤ ë„£ì–´ì¤˜."
    orchestrator.run_pipeline("session_dashboard_gamma", sample_request)
